{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f246cb-cb7b-4bba-91bb-5f221bf85ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, AdamW\n",
    "from collections import defaultdict\n",
    "from random import choice\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    句子最长长度是294 这里就不设参数限制长度了,每个batch 自适应长度\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.bert_path = '/data/sunyd/pretrained_models/bert-base-chinese/'\n",
    "\n",
    "\n",
    "        self.train_data_path = 'data/train.json'\n",
    "        self.dev_data_path = 'data/dev.json'\n",
    "        self.test_data_path = 'data/test.json'\n",
    "\n",
    "        self.batch_size = 1\n",
    "\n",
    "        self.rel_dict_path = 'data/rel.json'\n",
    "        self.id2rel = json.load(open(self.rel_dict_path, encoding='utf8'))\n",
    "        self.rel2id = {v: k for k, v in self.id2rel.items()} # 关系到id\n",
    "        self.num_rel = len(self.rel2id)  # 关系的种类数\n",
    "    \n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n",
    "\n",
    "        \n",
    "        self.learning_rate = 1e-5\n",
    "        self.bert_dim = 768\n",
    "        self.epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37d3120d-410b-45f4-970a-545aac50c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    #  batch是一个列表，其中是一个一个的元组，每个元组是dataset中_getitem__的结果\n",
    "    batch = list(zip(*batch))\n",
    "    text = batch[0]\n",
    "    triple = batch[1]\n",
    "    del batch\n",
    "    return text, triple\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        self.dataset = []\n",
    "        with open(path, encoding='utf8') as F:\n",
    "            for line in F:\n",
    "                line = json.loads(line)\n",
    "                self.dataset.append(line)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        content = self.dataset[item]\n",
    "        text = content['text']\n",
    "        spo_list = content['spo_list']\n",
    "        return text, spo_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "def create_data_iter(config):\n",
    "    train_data = MyDataset(config.train_data_path)\n",
    "    dev_data = MyDataset(config.dev_data_path)\n",
    "    test_data = MyDataset(config.test_data_path)\n",
    "\n",
    "    train_iter = DataLoader(train_data, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    dev_iter = DataLoader(dev_data, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    test_iter = DataLoader(test_data, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return train_iter, dev_iter, test_iter\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    def __init__(self, config):\n",
    "        self.tokenizer = config.tokenizer\n",
    "        self.num_relations = config.num_rel\n",
    "        self.rel2id = config.rel2id\n",
    "        self.device = config.device\n",
    "\n",
    "    def __call__(self, text, triple):\n",
    "        text = self.tokenizer(text, padding=True).data\n",
    "        batch_size = len(text['input_ids'])\n",
    "        seq_len = len(text['input_ids'][0])\n",
    "        sub_head = []\n",
    "        sub_tail = []\n",
    "        sub_heads = []\n",
    "        sub_tails = []\n",
    "        obj_heads = []\n",
    "        obj_tails = []\n",
    "        sub_len = []\n",
    "        sub_head2tail = []\n",
    "\n",
    "        for batch_index in range(batch_size):\n",
    "            inner_input_ids = text['input_ids'][batch_index]  # 单个句子变成索引后\n",
    "            inner_triples = triple[batch_index]\n",
    "            inner_sub_heads, inner_sub_tails, inner_sub_head, inner_sub_tail, inner_sub_head2tail, inner_sub_len, inner_obj_heads, inner_obj_tails = \\\n",
    "                self.create_label(inner_triples, inner_input_ids, seq_len)\n",
    "            sub_head.append(inner_sub_head)\n",
    "            sub_tail.append(inner_sub_tail)\n",
    "            sub_len.append(inner_sub_len)\n",
    "            sub_head2tail.append(inner_sub_head2tail)\n",
    "            sub_heads.append(inner_sub_heads)\n",
    "            sub_tails.append(inner_sub_tails)\n",
    "            obj_heads.append(inner_obj_heads)\n",
    "            obj_tails.append(inner_obj_tails)\n",
    "\n",
    "        input_ids = torch.tensor(text['input_ids']).to(self.device)\n",
    "        mask = torch.tensor(text['attention_mask']).to(self.device)\n",
    "        sub_head = torch.stack(sub_head).to(self.device)\n",
    "        sub_tail = torch.stack(sub_tail).to(self.device)\n",
    "        sub_heads = torch.stack(sub_heads).to(self.device)\n",
    "        sub_tails = torch.stack(sub_tails).to(self.device)\n",
    "        sub_len = torch.stack(sub_len).to(self.device)\n",
    "        sub_head2tail = torch.stack(sub_head2tail).to(self.device)\n",
    "        obj_heads = torch.stack(obj_heads).to(self.device)\n",
    "        obj_tails = torch.stack(obj_tails).to(self.device)\n",
    "        return {\n",
    "                   'input_ids': input_ids,\n",
    "                   'mask': mask,\n",
    "                   'sub_head2tail': sub_head2tail,\n",
    "                   'sub_len': sub_len\n",
    "               }, {\n",
    "                   'sub_heads': sub_heads,\n",
    "                   'sub_tails': sub_tails,\n",
    "                   'obj_heads': obj_heads,\n",
    "                   'obj_tails': obj_tails\n",
    "               }\n",
    "\n",
    "    def create_label(self, inner_triples, inner_input_ids, seq_len):\n",
    "\n",
    "        inner_sub_heads, inner_sub_tails = torch.zeros(seq_len), torch.zeros(seq_len)\n",
    "        inner_sub_head, inner_sub_tail = torch.zeros(seq_len), torch.zeros(seq_len)\n",
    "        \n",
    "        inner_obj_heads = torch.zeros((seq_len, self.num_relations))\n",
    "        inner_obj_tails = torch.zeros((seq_len, self.num_relations))\n",
    "        \n",
    "        inner_sub_head2tail = torch.zeros(seq_len)  # 随机抽取一个实体，从开头一个词到末尾词的索引\n",
    "\n",
    "        # 因为数据预处理代码还待优化,会有不存在关系三元组的情况，\n",
    "        # 初始化一个主词的长度为1，即没有主词默认主词长度为1，\n",
    "        # 防止零除报错,初始化任何非零数字都可以，没有主词分子是全零矩阵\n",
    "        inner_sub_len = torch.tensor([1], dtype=torch.float)\n",
    "        # 主词到谓词的映射\n",
    "        s2ro_map = defaultdict(list)\n",
    "        for inner_triple in inner_triples:\n",
    "\n",
    "            inner_triple = (\n",
    "                self.tokenizer(inner_triple['subject'], add_special_tokens=False)['input_ids'],\n",
    "                # self.rel_vocab.to_index(inner_triple['predicate']),\n",
    "                 eval(self.rel2id[inner_triple['predicate']]),\n",
    "                self.tokenizer(inner_triple['object'], add_special_tokens=False)['input_ids']\n",
    "            )\n",
    "\n",
    "            sub_head_idx = self.find_head_idx(inner_input_ids, inner_triple[0])\n",
    "            obj_head_idx = self.find_head_idx(inner_input_ids, inner_triple[2])\n",
    "\n",
    "            if sub_head_idx != -1 and obj_head_idx != -1:\n",
    "                sub = (sub_head_idx, sub_head_idx + len(inner_triple[0]) - 1)\n",
    "                # s2ro_map保存主语到谓语的映射\n",
    "                s2ro_map[sub].append(\n",
    "                    (obj_head_idx, obj_head_idx + len(inner_triple[2]) - 1, inner_triple[1]))  # {(3,5):[(7,8,0)]} 0是关系\n",
    "\n",
    "        if s2ro_map:\n",
    "            for s in s2ro_map:\n",
    "                inner_sub_heads[s[0]] = 1\n",
    "                inner_sub_tails[s[1]] = 1\n",
    "\n",
    "            sub_head_idx, sub_tail_idx = choice(list(s2ro_map.keys()))\n",
    "            inner_sub_head[sub_head_idx] = 1\n",
    "            inner_sub_tail[sub_tail_idx] = 1\n",
    "            inner_sub_head2tail[sub_head_idx:sub_tail_idx + 1] = 1\n",
    "            inner_sub_len = torch.tensor([sub_tail_idx + 1 - sub_head_idx], dtype=torch.float)\n",
    "            for ro in s2ro_map.get((sub_head_idx, sub_tail_idx), []):\n",
    "                inner_obj_heads[ro[0]][ro[2]] = 1\n",
    "                inner_obj_tails[ro[1]][ro[2]] = 1\n",
    "\n",
    "        return inner_sub_heads, inner_sub_tails, inner_sub_head, inner_sub_tail, inner_sub_head2tail, inner_sub_len, inner_obj_heads, inner_obj_tails\n",
    "\n",
    "    @staticmethod\n",
    "    def find_head_idx(source, target):\n",
    "        target_len = len(target)\n",
    "        for i in range(len(source)):\n",
    "            if source[i: i + target_len] == target:\n",
    "                return i\n",
    "        return -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83115839-eb45-43aa-8e58-6323eae41fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class CasRel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CasRel, self).__init__()\n",
    "        self.config = config\n",
    "        self.bert = BertModel.from_pretrained(self.config.bert_path)\n",
    "        self.sub_heads_linear = nn.Linear(self.config.bert_dim, 1)\n",
    "        self.sub_tails_linear = nn.Linear(self.config.bert_dim, 1)\n",
    "        self.obj_heads_linear = nn.Linear(self.config.bert_dim, self.config.num_rel)\n",
    "        self.obj_tails_linear = nn.Linear(self.config.bert_dim, self.config.num_rel)\n",
    "        self.alpha = 0.25\n",
    "        self.gamma = 2\n",
    "\n",
    "    def get_encoded_text(self, token_ids, mask):\n",
    "        encoded_text = self.bert(token_ids, attention_mask=mask)[0]\n",
    "        return encoded_text\n",
    "\n",
    "    def get_subs(self, encoded_text):\n",
    "        pred_sub_heads = torch.sigmoid(self.sub_heads_linear(encoded_text))\n",
    "        pred_sub_tails = torch.sigmoid(self.sub_tails_linear(encoded_text))\n",
    "        return pred_sub_heads, pred_sub_tails\n",
    "\n",
    "    def get_objs_for_specific_sub(self, sub_head2tail, sub_len, encoded_text):\n",
    "        # sub_head_mapping [batch, 1, seq] * encoded_text [batch, seq, dim]\n",
    "        sub = torch.matmul(sub_head2tail, encoded_text)  # batch size,1,dim\n",
    "        sub_len = sub_len.unsqueeze(1)\n",
    "        sub = sub / sub_len  # batch size, 1,dim\n",
    "        encoded_text = encoded_text + sub\n",
    "        #  [batch size, seq len,bert_dim] -->[batch size, seq len,relathion counts]\n",
    "        pred_obj_heads = torch.sigmoid(self.obj_heads_linear(encoded_text))\n",
    "        pred_obj_tails = torch.sigmoid(self.obj_tails_linear(encoded_text))\n",
    "        return pred_obj_heads, pred_obj_tails\n",
    "\n",
    "    def forward(self, input_ids, mask, sub_head2tail, sub_len):\n",
    "        \"\"\"\n",
    "\n",
    "        :param token_ids:[batch size, seq len]\n",
    "        :param mask:[batch size, seq len]\n",
    "        :param sub_head:[batch size, seq len]\n",
    "        :param sub_tail:[batch size, seq len]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        encoded_text = self.get_encoded_text(input_ids, mask)\n",
    "        pred_sub_heads, pred_sub_tails = self.get_subs(encoded_text)\n",
    "        sub_head2tail = sub_head2tail.unsqueeze(1)  # [[batch size,1, seq len]]\n",
    "        pred_obj_heads, pre_obj_tails = self.get_objs_for_specific_sub(sub_head2tail, sub_len, encoded_text)\n",
    "\n",
    "        return {\n",
    "            \"pred_sub_heads\": pred_sub_heads,\n",
    "            \"pred_sub_tails\": pred_sub_tails,\n",
    "            \"pred_obj_heads\": pred_obj_heads,\n",
    "            \"pred_obj_tails\": pre_obj_tails,\n",
    "            'mask': mask\n",
    "        }\n",
    "\n",
    "    def compute_loss(self, pred_sub_heads, pred_sub_tails, pred_obj_heads, pred_obj_tails, mask, sub_heads,\n",
    "                     sub_tails, obj_heads, obj_tails):\n",
    "        rel_count = obj_heads.shape[-1]\n",
    "        rel_mask = mask.unsqueeze(-1).repeat(1, 1, rel_count)\n",
    "        loss_1 = self.loss_fun(pred_sub_heads, sub_heads, mask)\n",
    "        loss_2 = self.loss_fun(pred_sub_tails, sub_tails, mask)\n",
    "        loss_3 = self.loss_fun(pred_obj_heads, obj_heads, rel_mask)\n",
    "        loss_4 = self.loss_fun(pred_obj_tails, obj_tails, rel_mask)\n",
    "        return loss_1 + loss_2 + loss_3 + loss_4\n",
    "\n",
    "    def loss_fun(self, logist, label, mask):\n",
    "        count = torch.sum(mask)\n",
    "        logist = logist.view(-1)\n",
    "        label = label.view(-1)\n",
    "        mask = mask.view(-1)\n",
    "        \n",
    "        alpha_factor = torch.where(torch.eq(label,1), 1- self.alpha,self.alpha)\n",
    "        focal_weight = torch.where(torch.eq(label,1),1-logist,logist)\n",
    "        \n",
    "        loss = -(torch.log(logist) * label + torch.log(1 - logist) * (1 - label)) * mask\n",
    "        return torch.sum(focal_weight * loss) / count\n",
    "    \n",
    "    def predict(self, input_ids, mask):\n",
    "        encoded_text = self.get_encoded_text(input_ids, mask)\n",
    "        pred_sub_heads, pred_sub_tails = self.get_subs(encoded_text)\n",
    "        pred_sub_heads = convert_score_to_zero_one(pred_sub_heads)\n",
    "        pred_sub_tails = convert_score_to_zero_one(pred_sub_tails)\n",
    "        subs = extract_sub(pred_sub_heads.squeeze(), pred_sub_tails.squeeze())\n",
    "        res = []\n",
    "        for sub in subs:\n",
    "            # print('sub: ', sub)\n",
    "            sub_text = ''.join(config.tokenizer.convert_ids_to_tokens(input_ids[0][sub[0]: sub[1] + 1]))\n",
    "            # print('sub_text:', sub_text)\n",
    "            sub_head2tail = torch.zeros(len(encoded_text[0]))\n",
    "            # print('sub_head2tail:', sub_head2tail)\n",
    "            sub_head2tail[sub[0]: sub[1] + 1] = 1\n",
    "            # print('sub_head2tail:', sub_head2tail)\n",
    "            sub_len = torch.tensor([sub[1] - sub[0] + 1])\n",
    "            pred_obj_heads, pred_obj_tails = self.get_objs_for_specific_sub(sub_head2tail, sub_len, encoded_text)\n",
    "            pred_obj_heads = convert_score_to_zero_one(pred_obj_heads)\n",
    "            pred_obj_tails = convert_score_to_zero_one(pred_obj_tails)\n",
    "            pred_ojbs = extract_obj_and_rel(pred_obj_heads.squeeze(), pred_obj_tails.squeeze())\n",
    "            for obj in pred_ojbs:\n",
    "                # print('obj:', obj)\n",
    "                obj_text = ''.join(config.tokenizer.convert_ids_to_tokens(input_ids[0][obj[1]: obj[2] + 1]))\n",
    "                # print('obj_text:', obj_text)\n",
    "                # print('relation:', config.id2rel[str(obj[0])])\n",
    "                res.append((config.id2rel[str(obj[0])], (sub_text, sub), (obj_text, obj[1: 3])))\n",
    "        \n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "166ed53e-3f95-443f-b0e0-06f756cd06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config):\n",
    "    device = config.device\n",
    "    model = CasRel(config)\n",
    "    model.to(device)\n",
    "\n",
    "    # prepare optimzier\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=config.learning_rate, eps=10e-8)\n",
    "    sheduler = None\n",
    "\n",
    "    return model, optimizer, sheduler, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cf5423c-6954-4cf6-a7a3-838d9295f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, train_iter, dev_iter, optimizer, batch, best_triple_f1, epoch):\n",
    "    for step, (text, triple) in enumerate(train_iter):\n",
    "        model.train()\n",
    "        inputs, labels = batch(text, triple)\n",
    "        logist = model(**inputs)\n",
    "        loss = model.compute_loss(**logist, **labels)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 500 == 1:\n",
    "            sub_precision, sub_recall, sub_f1, triple_precision, triple_recall, triple_f1, df = test(model, dev_iter,\n",
    "                                                                                                     batch)\n",
    "            if triple_f1 > best_triple_f1:\n",
    "                best_triple_f1 = triple_f1\n",
    "                torch.save(model.state_dict(), 'best_f1.pth')\n",
    "                print(\n",
    "                    'epoch:{},step:{},sub_precision:{:.4f}, sub_recall:{:.4f}, sub_f1:{:.4f}, triple_precision:{:.4f}, triple_recall:{:.4f}, triple_f1:{:.4f},train loss:{:.4f}'.format(\n",
    "                        epoch, step, sub_precision, sub_recall, sub_f1, triple_precision, triple_recall, triple_f1,\n",
    "                        loss.item()))\n",
    "                print(df)\n",
    "\n",
    "    return best_triple_f1\n",
    "\n",
    "\n",
    "def train(model, train_iter, dev_iter, optimizer, config):\n",
    "    epochs = config.epochs\n",
    "    best_triple_f1 = 0\n",
    "    for epoch in range(epochs):\n",
    "        best_triple_f1 = train_epoch(model, train_iter, dev_iter, optimizer, batch, best_triple_f1, epoch)\n",
    "\n",
    "\n",
    "def test(model, dev_iter, batch):\n",
    "    model.eval()\n",
    "    df = pd.DataFrame(columns=['TP', 'PRED', \"REAL\", 'p', 'r', 'f1'], index=['sub', 'triple'])\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    for text, triple in tqdm(dev_iter):\n",
    "        inputs, labels = batch(text, triple)\n",
    "        logist = model(**inputs)\n",
    "        \n",
    "        pred_sub_heads = convert_score_to_zero_one(logist['pred_sub_heads'])\n",
    "        pred_sub_tails = convert_score_to_zero_one(logist['pred_sub_tails'])\n",
    "\n",
    "        sub_heads = convert_score_to_zero_one(labels['sub_heads'])\n",
    "        sub_tails = convert_score_to_zero_one(labels['sub_tails'])\n",
    "        batch_size = inputs['input_ids'].shape[0]\n",
    "\n",
    "        obj_heads = convert_score_to_zero_one(labels['obj_heads'])\n",
    "        obj_tails = convert_score_to_zero_one(labels['obj_tails'])\n",
    "        pred_obj_heads = convert_score_to_zero_one(logist['pred_obj_heads'])\n",
    "        pred_obj_tails = convert_score_to_zero_one(logist['pred_obj_tails'])\n",
    "\n",
    "        for batch_index in range(batch_size):\n",
    "            pred_subs = extract_sub(pred_sub_heads[batch_index].squeeze(), pred_sub_tails[batch_index].squeeze())\n",
    "            true_subs = extract_sub(sub_heads[batch_index].squeeze(), sub_tails[batch_index].squeeze())\n",
    "\n",
    "            pred_ojbs = extract_obj_and_rel(pred_obj_heads[batch_index], pred_obj_tails[batch_index])\n",
    "            true_objs = extract_obj_and_rel(obj_heads[batch_index], obj_tails[batch_index])\n",
    "\n",
    "            df['PRED']['sub'] += len(pred_subs)\n",
    "            df['REAL']['sub'] += len(true_subs)\n",
    "            for true_sub in true_subs:\n",
    "                if true_sub in pred_subs:\n",
    "                    df['TP']['sub'] += 1\n",
    "\n",
    "            df['PRED']['triple'] += len(pred_ojbs)\n",
    "            df['REAL']['triple'] += len(true_objs)\n",
    "            for true_obj in true_objs:\n",
    "                if true_obj in pred_ojbs:\n",
    "                    df['TP']['triple'] += 1\n",
    "\n",
    "    df.loc['sub','p'] = df['TP']['sub'] / (df['PRED']['sub'] + 1e-9)\n",
    "    df.loc['sub','r'] = df['TP']['sub'] / (df['REAL']['sub'] + 1e-9)\n",
    "    df.loc['sub','f1'] = 2 * df['p']['sub'] * df['r']['sub'] / (df['p']['sub'] + df['r']['sub'] + 1e-9)\n",
    "    \n",
    "    sub_precision = df['TP']['sub'] / (df['PRED']['sub'] + 1e-9)\n",
    "    sub_recall = df['TP']['sub'] / (df['REAL']['sub'] + 1e-9)\n",
    "    sub_f1 = 2 * sub_precision * sub_recall  / (sub_precision + sub_recall  + 1e-9)\n",
    "\n",
    "    df.loc['triple','p'] = df['TP']['triple'] / (df['PRED']['triple'] + 1e-9)\n",
    "    df.loc['triple','r'] = df['TP']['triple'] / (df['REAL']['triple'] + 1e-9)\n",
    "    df.loc['triple','f1'] = 2 * df['p']['triple'] * df['r']['triple'] / (\n",
    "            df['p']['triple'] + df['r']['triple'] + 1e-9)\n",
    "    \n",
    "    \n",
    "    triple_precision = df['TP']['triple'] / (df['PRED']['triple'] + 1e-9)\n",
    "    triple_recall = df['TP']['triple'] / (df['REAL']['triple'] + 1e-9)\n",
    "    triple_f1 = 2 * triple_precision * triple_recall / (\n",
    "            triple_precision + triple_recall + 1e-9)\n",
    "\n",
    "    return sub_precision, sub_recall,sub_f1, triple_precision, triple_recall, triple_f1, df\n",
    "\n",
    "\n",
    "def extract_sub(pred_sub_heads, pred_sub_tails):\n",
    "    subs = []\n",
    "    heads = torch.arange(0, len(pred_sub_heads))[pred_sub_heads == 1]\n",
    "    tails = torch.arange(0, len(pred_sub_tails))[pred_sub_tails == 1]\n",
    "\n",
    "    for head, tail in zip(heads, tails):\n",
    "        if tail >= head:\n",
    "            subs.append((head.item(), tail.item()))\n",
    "    return subs\n",
    "\n",
    "\n",
    "def extract_obj_and_rel(obj_heads, obj_tails):\n",
    "    obj_heads = obj_heads.T\n",
    "    obj_tails = obj_tails.T\n",
    "    rel_count = obj_heads.shape[0]\n",
    "    obj_and_rels = []  # [(rel_index,strart_index,end_index),(rel_index,strart_index,end_index)]\n",
    "\n",
    "    for rel_index in range(rel_count):\n",
    "        obj_head = obj_heads[rel_index]\n",
    "        obj_tail = obj_tails[rel_index]\n",
    "\n",
    "        objs = extract_sub(obj_head, obj_tail)\n",
    "        if objs:\n",
    "            for obj in objs:\n",
    "                start_index, end_index = obj\n",
    "                obj_and_rels.append((rel_index, start_index, end_index))\n",
    "    return obj_and_rels\n",
    "\n",
    "\n",
    "def convert_score_to_zero_one(tensor):\n",
    "    tensor[tensor>=0.5] = 1\n",
    "    tensor[tensor<0.5] = 0\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a693756-8de5-4b6d-bb02-c6e52e1ebb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/sunyd/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [11:53<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,step:1,sub_precision:0.0124, sub_recall:0.0046, sub_f1:0.0067, triple_precision:0.0001, triple_recall:0.0112, triple_f1:0.0003,train loss:1.4684\n",
      "         TP     PRED   REAL         p         r        f1\n",
      "sub      54     4359  11759  0.012388  0.004592  0.006701\n",
      "triple  181  1418967  16225  0.000128  0.011156  0.000252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [08:50<00:00, 21.08it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [05:46<00:00, 32.26it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [10:13<00:00, 18.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,step:1501,sub_precision:0.8574, sub_recall:0.8600, sub_f1:0.8587, triple_precision:0.9248, triple_recall:0.0151, triple_f1:0.0298,train loss:0.0136\n",
      "           TP   PRED   REAL         p         r        f1\n",
      "sub     10113  11795  11759  0.857397  0.860022  0.858708\n",
      "triple    246    266  16239  0.924812  0.015149  0.029809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [09:16<00:00, 20.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,step:2001,sub_precision:0.8461, sub_recall:0.8768, sub_f1:0.8611, triple_precision:0.8163, triple_recall:0.1484, triple_f1:0.2511,train loss:0.0151\n",
      "           TP   PRED   REAL         p         r        f1\n",
      "sub     10310  12186  11759  0.846053  0.876775  0.861140\n",
      "triple   2408   2950  16229  0.816271  0.148376  0.251108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [06:42<00:00, 27.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,step:2501,sub_precision:0.8871, sub_recall:0.8724, sub_f1:0.8797, triple_precision:0.8246, triple_recall:0.1922, triple_f1:0.3117,train loss:0.0122\n",
      "           TP   PRED   REAL         p         r        f1\n",
      "sub     10258  11563  11759  0.887140  0.872353  0.879684\n",
      "triple   3121   3785  16240  0.824571  0.192180  0.311710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [10:15<00:00, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,step:3001,sub_precision:0.8889, sub_recall:0.8724, sub_f1:0.8806, triple_precision:0.7865, triple_recall:0.2714, triple_f1:0.4035,train loss:0.0107\n",
      "           TP   PRED   REAL         p         r        f1\n",
      "sub     10258  11540  11759  0.888908  0.872353  0.880553\n",
      "triple   4405   5601  16231  0.786467  0.271394  0.403536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [10:18<00:00, 18.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,step:3501,sub_precision:0.8972, sub_recall:0.8360, sub_f1:0.8655, triple_precision:0.7640, triple_recall:0.2907, triple_f1:0.4211,train loss:0.0052\n",
      "          TP   PRED   REAL         p         r        f1\n",
      "sub     9830  10956  11759  0.897225  0.835955  0.865507\n",
      "triple  4718   6175  16231  0.764049  0.290678  0.421137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [08:25<00:00, 22.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,step:4001,sub_precision:0.9011, sub_recall:0.8604, sub_f1:0.8803, triple_precision:0.6918, triple_recall:0.4415, triple_f1:0.5390,train loss:0.0027\n",
      "           TP   PRED   REAL         p         r        f1\n",
      "sub     10118  11229  11759  0.901060  0.860447  0.880285\n",
      "triple   7167  10360  16234  0.691795  0.441481  0.538994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [08:44<00:00, 21.36it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [10:15<00:00, 18.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,step:5001,sub_precision:0.9064, sub_recall:0.8633, sub_f1:0.8843, triple_precision:0.6938, triple_recall:0.4801, triple_f1:0.5675,train loss:0.0035\n",
      "           TP   PRED   REAL         p         r        f1\n",
      "sub     10151  11199  11759  0.906420  0.863254  0.884310\n",
      "triple   7796  11236  16237  0.693841  0.480138  0.567539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [10:09<00:00, 18.36it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:32<00:00, 41.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,step:6001,sub_precision:0.9026, sub_recall:0.8693, sub_f1:0.8856, triple_precision:0.6890, triple_recall:0.5404, triple_f1:0.6057,train loss:0.0026\n",
      "           TP   PRED   REAL         p         r        f1\n",
      "sub     10222  11325  11759  0.902605  0.869292  0.885635\n",
      "triple   8772  12732  16232  0.688973  0.540414  0.605717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:32<00:00, 41.06it/s]\n",
      " 40%|████████████████████████████████████████████▎                                                                 | 4513/11191 [01:50<02:45, 40.27it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:24<00:00, 42.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,step:10001,sub_precision:0.9435, sub_recall:0.8453, sub_f1:0.8917, triple_precision:0.7317, triple_recall:0.5638, triple_f1:0.6369,train loss:0.0034\n",
      "          TP   PRED   REAL         p         r        f1\n",
      "sub     9940  10535  11759  0.943522  0.845310  0.891720\n",
      "triple  9149  12504  16228  0.731686  0.563779  0.636851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|█████████████████████████████████████▏                                                                        | 3779/11191 [01:28<02:56, 41.94it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:24<00:00, 42.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,step:11501,sub_precision:0.9302, sub_recall:0.8672, sub_f1:0.8976, triple_precision:0.6617, triple_recall:0.6745, triple_f1:0.6680,train loss:0.0040\n",
      "           TP   PRED   REAL         p         r        f1\n",
      "sub     10197  10962  11759  0.930213  0.867166  0.897584\n",
      "triple  10949  16547  16232  0.661691  0.674532  0.668050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:23<00:00, 42.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,step:13501,sub_precision:0.9181, sub_recall:0.8792, sub_f1:0.8982, triple_precision:0.6736, triple_recall:0.7102, triple_f1:0.6914,train loss:0.1830\n",
      "           TP   PRED   REAL         p         r        f1\n",
      "sub     10338  11260  11759  0.918117  0.879156  0.898215\n",
      "triple  11532  17119  16237  0.673637  0.710230  0.691450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████████████████████████████████████████                                                         | 5401/11191 [02:07<02:15, 42.65it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:25<00:00, 42.19it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:26<00:00, 42.02it/s]\n",
      " 61%|███████████████████████████████████████████████████████████████████                                           | 6828/11191 [02:41<01:43, 42.06it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:25<00:00, 42.20it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:24<00:00, 42.26it/s]\n",
      " 29%|████████████████████████████████▎                                                                             | 3286/11191 [01:18<03:06, 42.44it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:25<00:00, 42.12it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:24<00:00, 42.28it/s]\n",
      " 15%|████████████████                                                                                              | 1631/11191 [00:38<03:42, 42.88it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:22<00:00, 42.66it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:24<00:00, 42.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,step:33001,sub_precision:0.9259, sub_recall:0.8931, sub_f1:0.9092, triple_precision:0.6797, triple_recall:0.7727, triple_f1:0.7232,train loss:0.0021\n",
      "           TP   PRED   REAL         p         r        f1\n",
      "sub     10502  11343  11759  0.925857  0.893103  0.909185\n",
      "triple  12539  18449  16227  0.679657  0.772724  0.723209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|███████████████████████████████▍                                                                              | 3201/11191 [01:16<03:09, 42.10it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:22<00:00, 42.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,step:39001,sub_precision:0.9530, sub_recall:0.8727, sub_f1:0.9111, triple_precision:0.7031, triple_recall:0.7775, triple_f1:0.7384,train loss:0.1434\n",
      "           TP   PRED   REAL         p         r        f1\n",
      "sub     10262  10768  11759  0.953009  0.872693  0.911084\n",
      "triple  12624  17955  16236  0.703091  0.777531  0.738440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|█████████████████████████████████████▏                                                                        | 3779/11191 [01:28<02:52, 43.04it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:24<00:00, 42.37it/s]\n",
      " 52%|████████████████████████████████████████████████████████▊                                                     | 5779/11191 [02:16<02:05, 43.24it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:24<00:00, 42.26it/s]\n",
      " 54%|███████████████████████████████████████████████████████████▋                                                  | 6074/11191 [02:24<02:01, 42.10it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:25<00:00, 42.10it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:25<00:00, 42.12it/s]\n",
      " 84%|████████████████████████████████████████████████████████████████████████████████████████████                  | 9366/11191 [03:42<00:43, 41.51it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:25<00:00, 42.23it/s]\n",
      " 44%|████████████████████████████████████████████████▋                                                             | 4947/11191 [01:57<02:28, 42.01it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:25<00:00, 42.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1,step:10001,sub_precision:0.9465, sub_recall:0.9050, sub_f1:0.9253, triple_precision:0.7152, triple_recall:0.7783, triple_f1:0.7454,train loss:0.0005\n",
      "           TP   PRED   REAL         p         r        f1\n",
      "sub     10642  11244  11759  0.946460  0.905009  0.925271\n",
      "triple  12647  17684  16249  0.715166  0.778325  0.745410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|█████████████████████████████████████████████████████████████████████████                                     | 7437/11191 [02:54<01:31, 41.12it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:26<00:00, 42.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:25<00:00, 42.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1,step:18001,sub_precision:0.9338, sub_recall:0.9230, sub_f1:0.9284, triple_precision:0.7024, triple_recall:0.8089, triple_f1:0.7519,train loss:0.0016\n",
      "           TP   PRED   REAL         p         r        f1\n",
      "sub     10853  11622  11759  0.933832  0.922953  0.928361\n",
      "triple  13135  18699  16239  0.702444  0.808855  0.751903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:25<00:00, 42.08it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:25<00:00, 42.17it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:26<00:00, 41.92it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:27<00:00, 41.91it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:24<00:00, 42.28it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:22<00:00, 42.69it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:23<00:00, 42.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:24<00:00, 42.26it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:24<00:00, 42.30it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:25<00:00, 42.23it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:23<00:00, 42.48it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:22<00:00, 42.58it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:24<00:00, 42.33it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:22<00:00, 42.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11191/11191 [04:23<00:00, 42.45it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    config = Config()\n",
    "    train_data = MyDataset(config.train_data_path)\n",
    "    model, optimizer, sheduler, device = load_model(config)\n",
    "    train_iter, dev_iter, test_iter = create_data_iter(config)\n",
    "    batch = Batch(config)\n",
    "    train(model, train_iter, dev_iter, optimizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28a219c0-f123-4465-b816-64bd513b721a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('导演', ('大话西游', (7, 10)), ('周星驰', (1, 3)))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "model = CasRel(config)\n",
    "model.load_state_dict(torch.load('best_f1.pth'))\n",
    "model.eval()\n",
    "sentence = '周星驰导演的大话西游很好看'\n",
    "inputs = config.tokenizer(sentence, return_tensors='pt')\n",
    "model.predict(inputs['input_ids'], inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9b1a5-bbe8-48fe-bef4-a5099f9f0a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9413442ce6ae88c42544d9271fa511dcee36605e0f25d5a494ac6efc84990a7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
